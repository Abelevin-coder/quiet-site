---
import BaseLayout from "../../layouts/BaseLayout.astro";

const title = "Why Capable People Decide Too Soon";
const subtitle = "Pressure and the Loss of Judgment";
const author = "Abe Levin";
const date = "2026";
---

<BaseLayout title={title}>
  <div class="essay-shell">
    <article class="essay-main dropcap">
      <h1>{title}</h1>

      {subtitle && <p><em>{subtitle}</em></p>}
      {author && date && <p><em>{author} · {date}</em></p>}

      <p>
        Some of the most consequential failures in modern history were made by people who knew
        better. They had information, experience, and clear standing within their domains of
        expertise. Still, they acted in ways that later proved difficult to account for. The
        question is why capable people move against their own understanding at precisely the
        moments when it should matter most.
      </p>

      <p>
        People often explain decision failure by pointing to bad information, time pressure, or
        misaligned incentives. Each of these explanations places the problem at the level of
        evaluation—how a situation was assessed, weighed, or interpreted at the moment of
        choice.
      </p>

      <p>
        These explanations are most visible in how major failures are publicly reconstructed—events
        that were widely scrutinized and assigned clear causes after the fact.
      </p>

      <p>
        In the months leading up to the U.S.-led invasion of Iraq in March 2003, senior American
        officials asserted that Saddam Hussein possessed weapons of mass destruction. Subsequent
        investigations concluded that key intelligence assessments were incorrect and poorly
        supported. The failure was widely described as an intelligence breakdown: flawed
        estimates, analytic groupthink, and the use of uncertain information as settled fact. The
        central error, in this account, lay in what decision-makers believed to be true.
      </p>

      <p>
        In the years preceding the global financial crisis of 2008, credit-rating agencies and
        financial institutions repeatedly assigned high ratings to complex mortgage-backed
        securities that later proved far riskier than advertised. Post-crisis analyses emphasized
        distorted incentives. Rating agencies were paid by issuers, while banks were rewarded for
        transaction volume rather than scrutiny. The crisis was framed as a failure of incentive
        alignment, where profit motives overwhelmed prudent risk assessment.
      </p>

      <p>
        The crashes of Boeing’s 737 MAX aircraft in October 2018 and March 2019 were attributed to
        engineering and regulatory failures. Investigations highlighted design flaws in the
        flight-control system and insufficient pilot training. The failure was framed as one of
        organizational incentives and process: speed prioritized over safety within engineering and
        regulatory systems.
      </p>

      <p>
        The loss of the space shuttle Columbia in February 2003 was analyzed as a case of
        cognitive bias and organizational failure. Engineers’ concerns about foam debris were
        dismissed by management, which relied on prior experience and optimistic assumptions.
        Subsequent reports cited groupthink, normalization of deviance, and institutional
        overconfidence.
      </p>

      <p>
        Russia’s invasion of Ukraine in February 2022 was widely explained as a strategic and
        ideological miscalculation. Analysts pointed to President Vladimir Putin’s belief that
        Ukraine lacked independent political legitimacy and would collapse quickly, and that
        Western response would prove fragmented and short-lived.
      </p>

      <p>
        During the Vietnam War of the 1960s, U.S. Secretary of Defense Robert McNamara relied
        heavily on quantitative metrics—body counts, sortie numbers, and statistical models—to
        assess progress. This approach has long been criticized as the misuse of managerial
        expertise in a political and cultural conflict, mistaking measurable outputs for
        strategic success.
      </p>

      <p>
        Taken together, these explanations aptly describe what people struggled with when a
        decision went wrong—information, incentives, beliefs, expertise, and assessments of risk.
        They explain failure at the point of weighing options. What they do not explain is what
        preceded that moment, when the decision could still be held open and then began to close.
      </p>

      <p>
        Before options are weighed, people are already carrying pressures that limit what can
        remain open. These pressures—time, responsibility, ambiguity, exposure, consequence—are
        not inputs to judgment. They are conditions that determine whether judgment can appear at
        all. They determine what feels urgent, what feels tolerable, what can still be held.
      </p>

      <p>
        When one of these pressures crosses a threshold, the system shifts. The person stops
        organizing around accuracy and begins organizing around relief. This is not a failure of
        discipline or intent. It is a change in state. Attention tightens. Delay starts to feel
        dangerous. What mattered moments earlier becomes harder to hold.
      </p>

      <p>
        From the outside, what follows often looks like decisiveness. A choice is made. A
        direction is set. From the inside, it carries a different quality. The decision feels
        inevitable. It arrives not because it is ready, but because remaining undecided has
        become intolerable.
      </p>

      <p>At that point, judgment has not been exercised; it has been displaced.</p>

      <p>
        One of the most serious attempts to address this pattern comes from Ray Dalio. In
        <em>Principles</em>, he begins from a clear observation: people tend to decide too early,
        especially under pressure. His response is to externalize judgment—slowing decisions
        through explicit rules, believability-weighted input, and structured processes designed to
        prevent premature closure.
      </p>

      <p>
        These methods are effective once a decision is already underway. They help regulate
        behavior after pressure has taken hold. What they do not address is the condition that
        makes early closure necessary in the first place. When pressure is already unregulated,
        judgment is not merely rushed; it is unavailable. In those moments, structure can delay
        action, but it cannot restore the internal conditions required for clarity to reappear.
      </p>

      <p>
        The implication is not that such frameworks are ineffective, but that they intervene after
        the decisive shift has already occurred.
      </p>

      <p>
        Re-examining the previous cases brings the point of formation into view—in the conditions
        under which decisions were forced to take shape.
      </p>

      <p>
        In the months leading up to the U.S.-led invasion of Iraq, what dominated the decision
        environment was not a lack of information, but the aftermath of September 11, 2001. Senior
        officials carried responsibility for preventing another catastrophic attack under
        conditions of sustained threat and public exposure. Delay itself became a liability. To
        remain undecided meant remaining exposed—to risk, to blame, to the possibility of being
        wrong too late. Under those conditions, the question could not stay open.
      </p>

      <p>
        Because the pressure surrounding delay could not be regulated, judgment narrowed and the
        decision closed while uncertainty was still present.
      </p>

      <p>
        In the years preceding the global financial crisis of 2008, delay carried exposure as
        surely as error. Staff inside banks and rating agencies worked through files at a
        relentless pace. Asset quality remained uncertain, but credit ratings made it possible to
        proceed once that uncertainty could no longer be carried.
      </p>

      <p>
        A similar pattern unfolded during the development of Boeing’s 737 MAX, certified in 2017.
        Engineers and managers worked under sustained competitive urgency. Delays carried
        visibility, explanation, and consequence. Concerns about unresolved risks were present,
        but holding them meant extending exposure—to executives, to regulators, to markets. As the
        pressure to move accumulated, judgment narrowed.
      </p>

      <p>
        Engineers working on the space shuttle Columbia project had raised unresolved questions
        about foam debris striking the wing. Acknowledging that uncertainty—about whether the
        damage could compromise re-entry—would have required visible disruption and exposure.
      </p>

      <p>
        When Russian forces invaded Ukraine, geopolitical isolation, legacy pressure, and a
        narrowing window of action shaped the decision environment. Prolonged ambiguity about
        Russia’s position and leverage in relation to Ukraine became difficult to hold, reducing
        the range of options that could remain open.
      </p>

      <p>
        The Vietnam War confronted Robert McNamara with persistent ambiguity and no clear markers
        of success. Numbers offered relief. They allowed decisions to proceed without waiting for
        clarity the situation itself could not provide.
      </p>

      <p>
        Across these cases, judgment became unavailable earlier, under conditions of unregulated
        tension. People encountered information, incentives, and bias only after that shift had
        already occurred. None of these determined whether judgment could appear. That
        determination happened earlier, at the level of regulation, before reasoning could
        operate.
      </p>

      <p>
        When this layer is missed, failure is misdiagnosed. Rules multiply, procedures tighten,
        and greater discipline is demanded of oneself and others. The load does not ease. Pressure
        rises. Compensation deepens. The same pattern repeats under a different explanation.
      </p>

      <p>
        Over time, the cost accumulates. Decisions continue to be demanded in states where
        judgment cannot appear. Escalation is mistaken for resolve. Closure is mistaken for
        clarity. Responsibility is assigned where capacity has already collapsed.
      </p>

      <p>
        People do not lose judgment only because they think badly or know too little. They fail
        when the tensions acting on a person exceed what can be regulated, and compensation
        replaces clarity. Until that condition is recognized, judgment will continue to be
        expected where it is structurally unavailable.
      </p>

      <h2>Endnotes</h2>

      <ol>
        <li>
          U.S. Senate Select Committee on Intelligence, <em>Report on the U.S. Intelligence
          Community’s Prewar Intelligence Assessments on Iraq</em>, 2004.
        </li>
        <li>
          Financial Crisis Inquiry Commission, <em>The Financial Crisis Inquiry Report</em>, 2011.
        </li>
        <li>
          Joint Authorities Technical Review (JATR), <em>Boeing 737 MAX Flight Control System</em>,
          2019; U.S. House Committee on Transportation and Infrastructure, <em>Final Committee
          Report</em>, 2020.
        </li>
      </ol>
    </article>

    <aside class="essay-aside">
      <div class="meta-block">
        <div class="meta-label">Essay</div>
        <div class="meta-title">{date}</div>
      </div>
    </aside>
  </div>
</BaseLayout>
